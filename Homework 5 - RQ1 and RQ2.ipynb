{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - Visit the Wikipedia hyperlinks graph!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import collections\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "import operator\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Researh question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RQ1] Build the graph [G=(V, E)] , where V is the set of articles and E the hyperlinks among them, and provide its basic information:\n",
    "\n",
    "* If it is direct or not\n",
    "* The number of nodes\n",
    "* The number of edges\n",
    "* The average node degree. Is the graph dense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading reduced edges file as dataframe, where one column is the source and other is the destination\n",
    "source_destination=pd.read_csv('wiki-topcats-reduced.txt',sep=\"\\t\",header=None,names=[\"source\",\"destination\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">reading of a dicionary created in _Homework 5 - RQ1 pre-check up.ipynb_ notebook file\n",
    "source_destination_dict is a **default dictionary** with list as datatype for value of dictionary, where **key**=source_id(article_id), **value**=destination_id(list of article_id's which are connected to that article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reading pickle file into a memory, which contains a dictionary where **key=source_id(article_id)**, **value=destination_id(list of article_id's** which are connected to that article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_destination_dict = pickle.load(open('source_destination_dict.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2645247it [02:21, 18686.07it/s]\n"
     ]
    }
   ],
   "source": [
    "#list of (source, destination) tuples which will be used to read edges from them in networkx _add_edges_from_ method\n",
    "source_destination_tuples=[(row[\"source\"],row[\"destination\"]) for idx,row in tqdm(source_destination.iterrows())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of nodes and edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results and the conclusion from the _Homework 5 - RQ1 pre-check up.ipynb_ we can use DiGraph() method to make a directed graph immediately. And check again the subquestions of the RQ1 in order to give the final response and conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 461193 \n",
      "Number of edges: 2645247\n"
     ]
    }
   ],
   "source": [
    "Gtup_directed = nx.DiGraph()\n",
    "Gtup_directed.add_edges_from(source_destination_tuples)\n",
    "print(\"Number of nodes:\",len(Gtup_directed.nodes()),\"\\nNumber of edges:\",len(Gtup_directed.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average node degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: DiGraph\n",
      "Number of nodes: 461193\n",
      "Number of edges: 2645247\n",
      "Average in degree:   5.7357\n",
      "Average out degree:   5.7357\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(Gtup_directed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Is the graph dense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density of the graph: 1.2436602635647606e-05\n"
     ]
    }
   ],
   "source": [
    "print('Density of the graph:',nx.density(Gtup_directed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The graph is a sparse graph. Because the density has a small value and it is closer to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of the RQ1:\n",
    "    \n",
    ">The graph is **DIRECTED**. That is why we used DiGraph() method from nx library\n",
    " \n",
    ">The **number of nodes**: 461 193\n",
    "\n",
    ">The **number of edges**: 2 645 247\n",
    "\n",
    ">The **average node degree** is: 5.7357\n",
    "\n",
    ">Is the graph dense? Density = 1.2437e-05 The density can have value from 0 to 1, whereby it is 0 for a graph without edges and 1 for a complete graph. Therefore we can conclude that **the graph is sparse** and not dense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Researh question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Building Block Ranking\n",
    "\n",
    "    > Based on the implementation of the **shortest path** algorithm compare sample number of nodes of C0-input category with all nodes in all the other Ci categories in order to build the **block ranking**. \n",
    "\n",
    "2. Ranking nodes of each category in the created block ranking vector and selecting top 3 and finding article names for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories file, where each category has a list of articles that belongs to it\n",
    "categories=pd.read_csv('wiki-topcats-categories.txt',sep=\"\\n\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_cat(row):\n",
    "   \n",
    "    \"\"\"\n",
    "    method which performs cleaning of each row from the categories file and counting the actual number of articles that belong to that category\n",
    "  \n",
    "    returns the number of articles \n",
    "    \"\"\"\n",
    "    return len(row.split(\"; \")[1].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the lambda and num_cat method for labeling and selecting categories who have more than 3500 articles\n",
    "categories=categories[categories.iloc[:,0].apply(lambda x:True if num_cat(x)>3500 else False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the dataframe with categories and mapping the articles to integers.\n",
    "#and making the categories_dict where key=category name, value=list of article id's belonging to that category\n",
    "categories_dict=defaultdict(list)\n",
    "for idx in categories.index:\n",
    "    cat_and_values=categories[0].loc[idx].split(\"; \")\n",
    "    cat_name=cat_and_values[0].split(\":\")[1]\n",
    "    categories_dict[cat_name]=list(map(int,cat_and_values[1].split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should consider as nodes, only the ones given in the reduced version of the graph. Therefore we are checking and removing\n",
    "#the other ones and based on that 6 more categories are eliminated. Therefore, now we have 29 categories.\n",
    "categories = {}\n",
    "all_nodes=set(Gtup_directed.nodes())\n",
    "for key, values in categories_dict.items():\n",
    "        categories[key] = all_nodes.intersection(set(values))\n",
    "        if len(categories[key]) < 3500:\n",
    "            del(categories[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to see how many edges every category has as part of the main directed graph and to choose the one that has the biggest number of edges so we can take a random sample of nodes in order to sort nodes in those categories in an easier way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_edges={}\n",
    "for cat in categories:\n",
    "    cat_edges[cat]=len(Gtup_directed.subgraph(categories[cat]).edges)We decided  to see how many edges every category has as part of the main directed graph and to choose the one that has the biggest number of edges so we can take a random sample of nodes in order to sort nodes in those categories in an easier way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'English_footballers': 18233,\n",
       " 'The_Football_League_players': 19672,\n",
       " 'Association_football_forwards': 4710,\n",
       " 'Association_football_goalkeepers': 8209,\n",
       " 'Association_football_midfielders': 3801,\n",
       " 'Association_football_defenders': 2286,\n",
       " 'Living_people': 1218406,\n",
       " 'Harvard_University_alumni': 3959,\n",
       " 'Major_League_Baseball_pitchers': 10472,\n",
       " 'Members_of_the_United_Kingdom_Parliament_for_English_constituencies': 30039,\n",
       " 'Indian_films': 3323,\n",
       " 'Year_of_death_missing': 1242,\n",
       " 'Year_of_birth_missing_(living_people)': 6814,\n",
       " 'Rivers_of_Romania': 15241,\n",
       " 'Main_Belt_asteroids': 10891,\n",
       " 'Asteroids_named_for_people': 142,\n",
       " 'English-language_albums': 7064,\n",
       " 'British_films': 2805,\n",
       " 'English-language_films': 21278,\n",
       " 'American_films': 10372,\n",
       " 'People_from_New_York_City': 3156,\n",
       " 'American_television_actors': 32231,\n",
       " 'American_film_actors': 53164,\n",
       " 'Debut_albums': 779,\n",
       " 'Black-and-white_films': 6288,\n",
       " 'Year_of_birth_missing': 1112,\n",
       " 'Place_of_birth_missing_(living_people)': 736,\n",
       " 'American_military_personnel_of_World_War_II': 5133,\n",
       " 'Windows_games': 6117}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('American_film_actors', 53164),\n",
       " ('American_films', 10372),\n",
       " ('American_military_personnel_of_World_War_II', 5133),\n",
       " ('American_television_actors', 32231),\n",
       " ('Association_football_defenders', 2286),\n",
       " ('Association_football_forwards', 4710),\n",
       " ('Association_football_goalkeepers', 8209),\n",
       " ('Association_football_midfielders', 3801),\n",
       " ('Asteroids_named_for_people', 142),\n",
       " ('Black-and-white_films', 6288),\n",
       " ('British_films', 2805),\n",
       " ('Debut_albums', 779),\n",
       " ('English-language_albums', 7064),\n",
       " ('English-language_films', 21278),\n",
       " ('English_footballers', 18233),\n",
       " ('Harvard_University_alumni', 3959),\n",
       " ('Indian_films', 3323),\n",
       " ('Living_people', 1218406),\n",
       " ('Main_Belt_asteroids', 10891),\n",
       " ('Major_League_Baseball_pitchers', 10472),\n",
       " ('Members_of_the_United_Kingdom_Parliament_for_English_constituencies',\n",
       "  30039),\n",
       " ('People_from_New_York_City', 3156),\n",
       " ('Place_of_birth_missing_(living_people)', 736),\n",
       " ('Rivers_of_Romania', 15241),\n",
       " ('The_Football_League_players', 19672),\n",
       " ('Windows_games', 6117),\n",
       " ('Year_of_birth_missing', 1112),\n",
       " ('Year_of_birth_missing_(living_people)', 6814),\n",
       " ('Year_of_death_missing', 1242)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sort descending to see category with most edges in its subgraph\n",
    "sorted(cat_edges.items(), key=operator.itemgetter(0), reverse= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In order to test and make a runnable version of our code, we decided to test it on the sample. We have chosen the **'American_film_actors'** as our **input category** and we will perform random sample, which is ok because it is a really sparse graph, and we decided to select **100 nodes** from the input category and **all the other nodes** from the rest of the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random sampling of 100 nodes from the input category 'American_film_actors' and making a sample subgraph for it\n",
    "# we repeat the process until at least 10 edges exist between sampled 100 nodes.\n",
    "for i in range(100):\n",
    "    cat_nodes_lst = random.sample(categories['American_film_actors'], 100)\n",
    "    Gtup_directed_sample_C0=Gtup_directed.subgraph(cat_nodes_lst)\n",
    "    if len(Gtup_directed_sample_C0.subgraph(categories['American_film_actors']).edges)>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53164"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of edges in whole subgraph for American_film_actors category\n",
    "len(Gtup_directed.subgraph(categories['American_film_actors']).edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name: \\nType: SubDiGraph\\nNumber of nodes: 100\\nNumber of edges: 11\\nAverage in degree:   0.1100\\nAverage out degree:   0.1100'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#11 edges exist\n",
    "nx.info(Gtup_directed_sample_C0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs_shortest_path(graph, start):\n",
    "    \"\"\"\n",
    "    method which returns shortest paths dictionary where key=node, and value is the actual distance of the shortest path from the start node to the other nodes of the graph\n",
    "    \"\"\"\n",
    "    explored = set()\n",
    "    queue = [start]\n",
    "    shortest_path_dict = {}\n",
    "    counter_level = 0\n",
    "    to_visit_list = []\n",
    "    while queue:\n",
    "        for node in queue:\n",
    "            if node not in explored:\n",
    "                shortest_path_dict[node] = counter_level\n",
    "                neighbours = graph[node]\n",
    "                explored.add(node)\n",
    "                to_visit_list.extend(neighbours)\n",
    "        queue = []\n",
    "        queue.extend(to_visit_list)\n",
    "        to_visit_list = []\n",
    "        counter_level += 1\n",
    "    return shortest_path_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:13<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "#calculate shortest path from each article of the SubGraph of the input category sample to all the other nodes in the Graph\n",
    "l = []\n",
    "for article in tqdm(Gtup_directed_sample_C0.nodes()):\n",
    "    l.append(bfs_shortest_path(Gtup_directed, article))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we made sample of 100 nodes from the input category now we have to remove other articles in the category **['American_film_actors'].**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to answer to the RQ2, we performed the following steps:\n",
    "\n",
    "1. Making of a dictionary **d** to store value of shortest paths, where key=node, value=list of shortest paths distances\n",
    "2. Making of a dictionary **z** to store value of **MINIMUM shortest path**, where key=node, value=minimum of the shortest path\n",
    "3. Making of a **categories_edited**, whose category **'American_film_actors'** will be consisted only of the **sampled articles**.\n",
    "4. Making of a **cat_inv_dic** where key=name of the category, value=shortest path values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d-dictionary, key=node, value=list of distances of all the possible shortest paths \n",
    "d = defaultdict(list)\n",
    "for node in l:\n",
    "    for key, value in node.items():\n",
    "        d[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z-dictionary, key=node, value=minimum distance of all the possible shortest paths \n",
    "z = defaultdict(list)\n",
    "for key, value in d.items():\n",
    "    z[key] = np.min(d[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories_edited will contain everything as categories except for the input category, which will contain only sampled nodes\n",
    "categories_edited = categories.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_delete_article list of articles which are not sampled, therefore not considered in testing and conclusion \n",
    "to_delete_article = []\n",
    "for article in categories_edited['American_film_actors']:\n",
    "    if article not in cat_nodes_lst:\n",
    "        to_delete_article.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deletion of other not sampled articles\n",
    "for article in to_delete_article:\n",
    "    categories_edited['American_film_actors'].remove(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see now 'American_film_actors' has **100 articles** the one that we sampled from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categories_edited['American_film_actors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#29 categories remained after elimination of ones that have less than 3500 articles\n",
    "len(categories_edited.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:02<00:00, 15.15it/s]\n"
     ]
    }
   ],
   "source": [
    "#cat_inv_dic, dictionary - key=name of the category, value= list of minimum shortest path values for its nodes(articles)\n",
    "cat_inv_dic=defaultdict(list)\n",
    "for cat_k,cat_v in tqdm(categories_edited.items()):\n",
    "    for node,val in z.items():\n",
    "        if node in cat_v:\n",
    "            cat_inv_dic[cat_k].append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking whether the number of sampled nodes in now right-100 as should be\n",
    "len(cat_inv_dic['American_film_actors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't consider the calculation of the shortest paths when 2 nodes are not connected, where the 'shortest path' would have been infinite. Since infinite doesn't make sense for median calculation we thought we could add a big number, such as 100.\n",
    "So where the edges between 2 nodes don't exist we extend it to values of 100 so it is considered in the median calculation.\n",
    "Of course, we checked how many inf values should be based on the number of articles that should belong to that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add value of 100 where nodes are not connected\n",
    "for cat_k in categories_edited.keys():\n",
    "    cat_inv_dic[cat_k].extend([100]*(len(categories_edited[cat_k])-len(cat_inv_dic[cat_k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the median\n",
    "for key, value in cat_inv_dic.items():\n",
    "    cat_inv_dic[key] = statistics.median(cat_inv_dic[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'English_footballers': 6.0,\n",
       "             'The_Football_League_players': 6.0,\n",
       "             'Association_football_forwards': 7,\n",
       "             'Association_football_goalkeepers': 100,\n",
       "             'Association_football_midfielders': 100,\n",
       "             'Association_football_defenders': 11.0,\n",
       "             'Living_people': 5.0,\n",
       "             'Harvard_University_alumni': 5,\n",
       "             'Major_League_Baseball_pitchers': 5.0,\n",
       "             'Members_of_the_United_Kingdom_Parliament_for_English_constituencies': 5,\n",
       "             'Indian_films': 4.0,\n",
       "             'Year_of_death_missing': 100.0,\n",
       "             'Year_of_birth_missing_(living_people)': 5.0,\n",
       "             'Rivers_of_Romania': 6,\n",
       "             'Main_Belt_asteroids': 100.0,\n",
       "             'Asteroids_named_for_people': 100,\n",
       "             'English-language_albums': 4.0,\n",
       "             'British_films': 3.0,\n",
       "             'English-language_films': 3,\n",
       "             'American_films': 3,\n",
       "             'People_from_New_York_City': 4.0,\n",
       "             'American_television_actors': 3,\n",
       "             'American_film_actors': 0.0,\n",
       "             'Debut_albums': 4,\n",
       "             'Black-and-white_films': 3,\n",
       "             'Year_of_birth_missing': 100.0,\n",
       "             'Place_of_birth_missing_(living_people)': 5.0,\n",
       "             'American_military_personnel_of_World_War_II': 5.0,\n",
       "             'Windows_games': 5})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cat_inv_dic, where key=name of the category, value is the calculated median\n",
    "cat_inv_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort descending\n",
    "cat_inv_dic_rank= sorted(cat_inv_dic.items(), key=operator.itemgetter(1), reverse= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('American_film_actors', 0.0),\n",
       " ('British_films', 3.0),\n",
       " ('English-language_films', 3),\n",
       " ('American_films', 3),\n",
       " ('American_television_actors', 3),\n",
       " ('Black-and-white_films', 3),\n",
       " ('Indian_films', 4.0),\n",
       " ('English-language_albums', 4.0),\n",
       " ('People_from_New_York_City', 4.0),\n",
       " ('Debut_albums', 4),\n",
       " ('Living_people', 5.0),\n",
       " ('Harvard_University_alumni', 5),\n",
       " ('Major_League_Baseball_pitchers', 5.0),\n",
       " ('Members_of_the_United_Kingdom_Parliament_for_English_constituencies', 5),\n",
       " ('Year_of_birth_missing_(living_people)', 5.0),\n",
       " ('Place_of_birth_missing_(living_people)', 5.0),\n",
       " ('American_military_personnel_of_World_War_II', 5.0),\n",
       " ('Windows_games', 5),\n",
       " ('English_footballers', 6.0),\n",
       " ('The_Football_League_players', 6.0),\n",
       " ('Rivers_of_Romania', 6),\n",
       " ('Association_football_forwards', 7),\n",
       " ('Association_football_defenders', 11.0),\n",
       " ('Association_football_goalkeepers', 100),\n",
       " ('Association_football_midfielders', 100),\n",
       " ('Year_of_death_missing', 100.0),\n",
       " ('Main_Belt_asteroids', 100.0),\n",
       " ('Asteroids_named_for_people', 100),\n",
       " ('Year_of_birth_missing', 100.0)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Block ranking vector\n",
    "cat_inv_dic_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Category name', 'Median']\n",
    "\n",
    "block_ranking_df = pd.DataFrame([x for x in cat_inv_dic_rank], columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_ranking_df.index.names = ['Ranking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category name</th>\n",
       "      <th>Median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ranking</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>American_film_actors</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>British_films</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English-language_films</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>American_films</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>American_television_actors</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Black-and-white_films</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Indian_films</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>English-language_albums</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>People_from_New_York_City</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Debut_albums</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Living_people</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Harvard_University_alumni</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Major_League_Baseball_pitchers</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Members_of_the_United_Kingdom_Parliament_for_E...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Year_of_birth_missing_(living_people)</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Place_of_birth_missing_(living_people)</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>American_military_personnel_of_World_War_II</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Windows_games</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>English_footballers</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The_Football_League_players</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Rivers_of_Romania</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Association_football_forwards</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Association_football_defenders</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Association_football_goalkeepers</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Association_football_midfielders</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Year_of_death_missing</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Main_Belt_asteroids</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Asteroids_named_for_people</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Year_of_birth_missing</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Category name  Median\n",
       "Ranking                                                           \n",
       "0                                     American_film_actors     0.0\n",
       "1                                            British_films     3.0\n",
       "2                                   English-language_films     3.0\n",
       "3                                           American_films     3.0\n",
       "4                               American_television_actors     3.0\n",
       "5                                    Black-and-white_films     3.0\n",
       "6                                             Indian_films     4.0\n",
       "7                                  English-language_albums     4.0\n",
       "8                                People_from_New_York_City     4.0\n",
       "9                                             Debut_albums     4.0\n",
       "10                                           Living_people     5.0\n",
       "11                               Harvard_University_alumni     5.0\n",
       "12                          Major_League_Baseball_pitchers     5.0\n",
       "13       Members_of_the_United_Kingdom_Parliament_for_E...     5.0\n",
       "14                   Year_of_birth_missing_(living_people)     5.0\n",
       "15                  Place_of_birth_missing_(living_people)     5.0\n",
       "16             American_military_personnel_of_World_War_II     5.0\n",
       "17                                           Windows_games     5.0\n",
       "18                                     English_footballers     6.0\n",
       "19                             The_Football_League_players     6.0\n",
       "20                                       Rivers_of_Romania     6.0\n",
       "21                           Association_football_forwards     7.0\n",
       "22                          Association_football_defenders    11.0\n",
       "23                        Association_football_goalkeepers   100.0\n",
       "24                        Association_football_midfielders   100.0\n",
       "25                                   Year_of_death_missing   100.0\n",
       "26                                     Main_Belt_asteroids   100.0\n",
       "27                              Asteroids_named_for_people   100.0\n",
       "28                                   Year_of_birth_missing   100.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_ranking_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see 'American_film_actors' is our input category and it has median 0 which is logical cause it is an input category and shortest distances are 0 between that category and its nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the names of the articles from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#article_names_dict, dictionary, where key=article id, value=article name\n",
    "article_names_dict={}\n",
    "with open('wiki-topcats-page-names.txt') as file:\n",
    "    for line in file:\n",
    "        id_name=line.rstrip().split(' ')\n",
    "        article_names_dict[int(id_name[0])]=\" \".join(id_name[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chiasmal syndrome'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of first article\n",
    "article_names_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of ranked categories\n",
    "cat_lst=list(block_ranking_df['Category name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation of the subgraph induced by input category-C0. For each node the sum of **the weigths of the in-edges** should be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_category=cat_lst[0]\n",
    "\n",
    "sub_graph_Cat=Gtup_directed.subgraph((list(categories_edited[input_category])))#make a subgraph containing just THAT category\n",
    "in_edge_All=[] #list of first 3 sorted nodes for each category \n",
    "dict_previous_cat={} #dictionary containing a previously ranked category of the current category,key=node, value=number of in-edges \n",
    "\n",
    "#just for first category-input category C0\n",
    "in_edge_temp_dict = defaultdict(int) #temporary dictionary to store as key=node, value=number of in-edges \n",
    "for tupla in list(sub_graph_Cat.edges()): # tupla-edge tuple, (source node, destination node)\n",
    "    if tupla[1] in categories_dict[input_category]: # if the destination node belongs to the current C0 category\n",
    "        in_edge_temp_dict[tupla[1]] += 1 #add one to that destination node in order to count all the in-edges\n",
    "\n",
    "sorted_in_edge_dict= sorted(in_edge_temp_dict.items(), key=operator.itemgetter(1), reverse= True) # sort the nodes in that category\n",
    "in_edge_All.append(sorted_in_edge_dict[0:3]) #add sorted dictionary of nodes to a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1163611, 2), (1062014, 2), (1163947, 1)]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top 3 sorted nodes from the first input category\n",
    "in_edge_All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "#number of edges in the sampled input category\n",
    "print(len(list(sub_graph_Cat.edges())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 28/28 [00:09<00:00,  3.01it/s]\n"
     ]
    }
   ],
   "source": [
    "in_edge_dict=in_edge_temp_dict #take nodes and values from the 1st input category\n",
    "for cat in tqdm(cat_lst[1:]):#for other 28 categories\n",
    "    in_edge_temp_dict = defaultdict(int)\n",
    "    sub_graph_Cat=Gtup_directed.subgraph((list(categories_edited[cat])))#make a subgraph containing just THAT category\n",
    "    for tupla in list(sub_graph_Cat.edges()): # go to every node of the cat subgraph\n",
    "        try:\n",
    "            source=in_edge_dict[tupla[0]] #take the source node from the previous category\n",
    "            in_edge_temp_dict[tupla[1]] += source #add in-edge value from the previous node\n",
    "        except:\n",
    "            in_edge_temp_dict[tupla[1]] += 1 #append 1 if it is not there\n",
    "    in_edge_dict.update(in_edge_temp_dict)\n",
    "\n",
    "    sorted_in_edge_dict= sorted(in_edge_temp_dict.items(), key=operator.itemgetter(1), reverse= True)\n",
    "    in_edge_All.append(sorted_in_edge_dict[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(in_edge_All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1163611, 2), (1062014, 2), (1163947, 1)],\n",
       " [(1056770, 0), (1056769, 0), (1041937, 0)],\n",
       " [(1062743, 0), (1064184, 0), (1064265, 0)],\n",
       " [(1245946, 0), (688149, 0), (688300, 0)],\n",
       " [(1163623, 2), (1061808, 2), (1061750, 2)],\n",
       " [(589847, 0), (590961, 0), (1063431, 0)],\n",
       " [(582698, 0), (590800, 0), (589828, 0)],\n",
       " [(1228798, 0), (1228799, 0), (145526, 0)],\n",
       " [(1061452, 2), (1023762, 2), (1164816, 2)],\n",
       " [(566701, 0), (328881, 0), (155018, 0)],\n",
       " [(1062014, 12), (1061284, 8), (1061865, 8)],\n",
       " [(1062012, 4), (1380119, 4), (1400478, 2)],\n",
       " [(386079, 0), (384805, 0), (387201, 0)],\n",
       " [(543595, 1), (540950, 1), (537478, 1)],\n",
       " [(643206, 0), (973046, 0), (655275, 0)],\n",
       " [(1151940, 0), (491635, 0), (1562881, 0)],\n",
       " [(1164934, 8), (1061441, 8), (1025555, 8)],\n",
       " [(1734268, 0), (1507349, 0), (1735504, 0)],\n",
       " [(80379, 0), (87391, 0), (78944, 0)],\n",
       " [(81787, 0), (81941, 0), (82484, 0)],\n",
       " [(785340, 0), (786385, 0), (786433, 0)],\n",
       " [(1358244, 0), (88262, 0), (884966, 0)],\n",
       " [(79221, 0), (81524, 0), (81919, 0)],\n",
       " [(737293, 0), (81952, 0), (81104, 0)],\n",
       " [(78944, 0), (88754, 0), (82393, 0)],\n",
       " [(1048577, 0), (1048576, 0), (1048578, 0)],\n",
       " [(870589, 0), (870768, 0), (1249929, 0)],\n",
       " [(874496, 0), (871119, 0), (871248, 0)],\n",
       " [(1048552, 0), (1048577, 0), (1048582, 0)]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_edge_All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 American_film_actors [('John Payne (actor)', 2), ('Christian Slater', 2), ('Meredith Baxter', 1)]\n",
      "1 British_films [('The Golden Voyage of Sinbad', 0), ('Sinbad and the Eye of the Tiger', 0), ('Cinema of the United Kingdom', 0)]\n",
      "2 English-language_films [('Reservoir Dogs', 0), ('Predators (film)', 0), ('Machete (film)', 0)]\n",
      "3 American_films [('Educating Peter', 0), ('Behind the Green Door', 0), ('Deep Throat (film)', 0)]\n",
      "4 American_television_actors [('George Burns', 2), ('Emilio Estevez', 2), ('Michael J. Fox', 2)]\n",
      "5 Black-and-white_films [('Ladki', 0), ('Penn (film)', 0), ('Duck Soup (1933 film)', 0)]\n",
      "6 Indian_films [('Shola Aur Shabnam (1992 film)', 0), ('Illarikam', 0), ('Aasha (1957 film)', 0)]\n",
      "7 English-language_albums [('Bloodrock U.S.A.', 0), ('Bloodrock 2', 0), ('Wild in the Streets (Circle Jerks album)', 0)]\n",
      "8 People_from_New_York_City [('Lauren Bacall', 2), ('Artie Shaw', 2), ('William S. Paley', 2)]\n",
      "9 Debut_albums [('America (America album)', 0), ('No One Can Do It Better', 0), ('The Scream (album)', 0)]\n",
      "10 Living_people [('Christian Slater', 12), ('Roger Ebert', 8), ('John Travolta', 8)]\n",
      "11 Harvard_University_alumni [('Natalie Portman', 4), ('Sean Gullette', 4), ('John F. Kennedy', 2)]\n",
      "12 Major_League_Baseball_pitchers [('Sparky Lyle', 0), ('Curt Simmons', 0), ('Joe Niekro', 0)]\n",
      "13 Members_of_the_United_Kingdom_Parliament_for_English_constituencies [('Robert Jenkinson, 2nd Earl of Liverpool', 1), ('Winston Churchill', 1), ('Robert Gascoyne-Cecil, 3rd Marquess of Salisbury', 1)]\n",
      "14 Year_of_birth_missing_(living_people) [('Andy Wellings', 0), ('Jean Donnelly', 0), ('Laci Scott', 0)]\n",
      "15 Place_of_birth_missing_(living_people) [('Steve Holland (writer)', 0), ('Pamela Spencer', 0), ('Jan Wohlschlag', 0)]\n",
      "16 American_military_personnel_of_World_War_II [('Mike Wallace (journalist)', 8), ('Charlton Heston', 8), ('Tony Bennett', 8)]\n",
      "17 Windows_games [('Angelique (video game)', 0), ('Everquest: The Shadows of Luclin', 0), ('EverQuest', 0)]\n",
      "18 English_footballers [('Stuart Ripley', 0), ('Jason Steele (footballer)', 0), ('Kyle Bennett (footballer)', 0)]\n",
      "19 The_Football_League_players [('Mick McCarthy', 0), ('Glenn Hoddle', 0), ('Ian Harte', 0)]\n",
      "20 Rivers_of_Romania [('Lona River', 0), ('Cerna River (Mure)', 0), ('Sterminos River (Cerna)', 0)]\n",
      "21 Association_football_forwards [('Christian Stuani', 0), ('Alessandro Altobelli', 0), ('Fernando Morena', 0)]\n",
      "22 Association_football_defenders [('Aaron Lescott', 0), ('Sylvain Distin', 0), ('Phil Jagielka', 0)]\n",
      "23 Association_football_goalkeepers [('Jos Pereira', 0), ('Goalkeeper (association football)', 0), ('Maik Taylor', 0)]\n",
      "24 Association_football_midfielders [('Kyle Bennett (footballer)', 0), ('Simone Perrotta', 0), ('David Beckham', 0)]\n",
      "25 Year_of_death_missing [('Richard Witton', 0), ('Thomas Benwell', 0), ('Robert Burton (academic)', 0)]\n",
      "26 Main_Belt_asteroids [('Asteroid belt', 0), ('19367 Pink Floyd', 0), ('3237 Victorplatt', 0)]\n",
      "27 Asteroids_named_for_people [('6075 Zajtsev', 0), ('153 Hilda', 0), ('3749 Balam', 0)]\n",
      "28 Year_of_birth_missing [('Thomas Caius', 0), ('Richard Witton', 0), ('John Martyn (academic)', 0)]\n"
     ]
    }
   ],
   "source": [
    "#print the block ranking vector and top 3 articles which belong to each of the category\n",
    "for idx,row in enumerate(in_edge_All):\n",
    "    row_lst=[]\n",
    "    for tup in row:\n",
    "        tuple_new=(article_names_dict[tup[0]],tup[1])\n",
    "        row_lst.append(tuple_new)\n",
    "    print(idx,cat_lst[idx],row_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our input category **American_film_actors** based on the random sample the most popular article is about **'Elvis Presley'**, who played in 31 movies as an actor ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
